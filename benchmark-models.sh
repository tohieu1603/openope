#!/bin/bash
# ============================================================
# OpenClaw Model Benchmark
# 20 real-world workflows: Basic ‚Üí Intermediate ‚Üí Advanced
# Based on actual OpenClaw tools: exec, read/write/edit, web_search,
# web_fetch, browser, message, cron, sessions_spawn, memory, image
# ============================================================

set -euo pipefail

RESULTS_DIR="./benchmark-results"
mkdir -p "$RESULTS_DIR"

echo "============================================"
echo " OpenClaw Model Benchmark - $(date '+%Y-%m-%d %H:%M')"
echo "============================================"

python3 << 'PYEOF'
import json, time, os, urllib.request

GATEWAY_URL = "http://localhost:3000"
_home = os.path.expanduser("~")
_cfg = json.load(open(os.path.join(_home, ".openclaw/openclaw.json")))
GW_TOKEN = _cfg.get("gateway", {}).get("auth", {}).get("token", "")
RESULTS_DIR = "./benchmark-results"
TIMEOUT = 120
MAX_TOKENS = 2048

MODELS = [
    "deepseek-chat",
    "byteplus/kimi-k2.5",
    "byteplus/kimi-k2-thinking",
    "byteplus/gpt-oss-120b",
    "byteplus/glm-4.7",
    "byteplus/bytedance-seed-code",
]

# ============================================================
# 20 Real OpenClaw Workflows ‚Äî Basic ‚Üí Intermediate ‚Üí Advanced
#
# BASIC (1-7): Daily chat, Q&A, Vietnamese, simple tasks
#   - User chats via Telegram/Zalo in Vietnamese
#   - Simple questions, translations, formatting
#
# INTERMEDIATE (8-14): Coding, content, data tasks
#   - Write/debug code (coding-agent workflow)
#   - Summarize, extract data, analyze errors
#
# ADVANCED (15-20): Multi-tool orchestration, automation
#   - Tool call generation, cron scheduling
#   - Complex coding, multi-step planning
# ============================================================

QUESTIONS = [
    # ======== BASIC (1-7): Daily user chat ========

    # User chats via Zalo ‚Äî basic Vietnamese greeting
    (1, "Basic",
     "Ch√†o b·∫°n! M√¨nh l√† Minh, m√¨nh m·ªõi d√πng Operis. B·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu b·∫°n l√† ai v√† b·∫°n c√≥ th·ªÉ gi√∫p m√¨nh nh·ªØng g√¨ kh√¥ng?",
     "Operis"),

    # User asks simple tech question via Telegram
    (2, "Basic",
     "REST API v√† GraphQL kh√°c nhau ch·ªó n√†o? Tr·∫£ l·ªùi ng·∫Øn g·ªçn 3 ƒëi·ªÉm ch√≠nh.",
     "GraphQL"),

    # User asks to explain code (read tool context)
    (3, "Basic",
     "Gi·∫£i th√≠ch ƒëo·∫°n code n√†y l√†m g√¨:\n```javascript\nconst pipe = (...fns) => x => fns.reduce((v, f) => f(v), x);\n```",
     "reduce"),

    # User asks to translate (message tool ‚Äî cross-channel)
    (4, "Basic",
     "D·ªãch sang ti·∫øng Anh chuy√™n nghi·ªáp gi√∫p m√¨nh:\n'H·ªá th·ªëng s·∫Ω b·∫£o tr√¨ t·ª´ 22h ng√†y 15/02 ƒë·∫øn 6h ng√†y 16/02. Trong th·ªùi gian n√†y d·ªãch v·ª• t·∫°m ng∆∞ng. Xin l·ªói v√¨ s·ª± b·∫•t ti·ªán.'",
     "maintenance"),

    # User asks to format data (write tool context)
    (5, "Basic",
     "Format d·ªØ li·ªáu n√†y th√†nh markdown table:\nMinh - Backend Dev - Platform team\nLan - Frontend Dev - Product team\nƒê·ª©c - DevOps - Infrastructure team",
     "| Minh"),

    # User asks about system capabilities
    (6, "Basic",
     "M√¨nh mu·ªën bi·∫øt b·∫°n c√≥ th·ªÉ gi√∫p m√¨nh nh·ªØng g√¨? Li·ªát k√™ 5 vi·ªác ch√≠nh b·∫°n l√†m ƒë∆∞·ª£c.",
     "code"),

    # User asks simple math/logic
    (7, "Basic",
     "Server m√¨nh nh·∫≠n 500 request/gi√¢y, m·ªói request x·ª≠ l√Ω 100ms. C·∫ßn t·ªëi thi·ªÉu bao nhi√™u thread ƒë·ªÉ x·ª≠ l√Ω h·∫øt? Gi·∫£i th√≠ch ng·∫Øn g·ªçn.",
     "50"),

    # ======== INTERMEDIATE (8-14): Coding & content ========

    # Coding: write a function (exec/write tool context)
    (8, "Mid",
     "Vi·∫øt function TypeScript t√™n `debounce` nh·∫≠n callback v√† delay(ms), tr·∫£ v·ªÅ debounced function. Y√™u c·∫ßu: generic typing, gi·ªØ this context, c√≥ JSDoc. Ch·ªâ output code.",
     "debounce"),

    # Coding: debug (read/edit tool context)
    (9, "Mid",
     "T√¨m bug trong code Python n√†y v√† s·ª≠a:\n```python\ndef merge_sorted(a, b):\n    result = []\n    i = j = 0\n    while i < len(a) and j < len(b):\n        if a[i] <= b[j]:\n            result.append(a[i])\n            i += 1\n        else:\n            result.append(b[j])\n            j += 1\n    return result\n```\nGi·∫£i th√≠ch bug v√† cho code ƒë√£ s·ª≠a.",
     "remaining"),

    # Content: write welcome message (message tool)
    (10, "Mid",
     "Vi·∫øt welcome message cho Telegram bot c·ªßa Operis. Y√™u c·∫ßu:\n- Ch√†o user th√¢n thi·ªán\n- Gi·ªõi thi·ªáu 3 t√≠nh nƒÉng: h·ªó tr·ª£ coding, t·ª± ƒë·ªông h√≥a, ƒëa ng√¥n ng·ªØ\n- D√πng emoji ph√π h·ª£p\n- K·∫øt th√∫c b·∫±ng call-to-action\n- T·ªëi ƒëa 120 t·ª´, gi·ªçng vƒÉn chuy√™n nghi·ªáp nh∆∞ng g·∫ßn g≈©i",
     "Operis"),

    # Data extraction (web_fetch context ‚Äî extract from text)
    (11, "Mid",
     "Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ text sau th√†nh JSON {name, role, company, action, deadline}:\n\n'Xin ch√†o team, t√¥i l√† Nguy·ªÖn VƒÉn A t·ª´ c√¥ng ty VNG. V·ªõi vai tr√≤ Lead DevOps, t√¥i c·∫ßn m·ªçi ng∆∞·ªùi ho√†n th√†nh vi·ªác migrate CI/CD pipeline sang GitHub Actions tr∆∞·ªõc ng√†y 15/03/2026.'",
     "Nguyen"),

    # Error analysis (exec/read context ‚Äî analyze logs)
    (12, "Mid",
     "Ph√¢n t√≠ch l·ªói API n√†y v√† ƒë·ªÅ xu·∫•t 3 c√°ch fix c·ª• th·ªÉ:\n```json\n{\"status\": 429, \"error\": \"Too Many Requests\", \"headers\": {\"retry-after\": \"30\", \"x-ratelimit-limit\": \"100\", \"x-ratelimit-remaining\": \"0\"}, \"body\": {\"message\": \"Rate limit exceeded. Implement exponential backoff.\"}}\n```",
     "backoff"),

    # Summarize (web_fetch/read context)
    (13, "Mid",
     "T√≥m t·∫Øt ƒë√∫ng 3 bullet points (m·ªói point t·ªëi ƒëa 20 t·ª´):\n\nKubernetes l√† n·ªÅn t·∫£ng container orchestration m√£ ngu·ªìn m·ªü t·ª± ƒë·ªông h√≥a tri·ªÉn khai, m·ªü r·ªông v√† qu·∫£n l√Ω ·ª©ng d·ª•ng container. N√≥ nh√≥m container th√†nh pod ƒë·ªÉ d·ªÖ qu·∫£n l√Ω. Kubernetes cung c·∫•p self-healing - t·ª± restart container l·ªói, thay th·∫ø container khi node ch·∫øt. N√≥ h·ªó tr·ª£ horizontal scaling t·ª± ƒë·ªông ho·∫∑c th·ªß c√¥ng d·ª±a tr√™n CPU. Service discovery v√† load balancing t√≠ch h·ª£p s·∫µn qua DNS ho·∫∑c IP.",
     "pod"),

    # Vietnamese technical explanation
    (14, "Mid",
     "Gi·∫£i th√≠ch 'Infrastructure as Code' cho developer Vi·ªát Nam m·ªõi b·∫Øt ƒë·∫ßu. Bao g·ªìm:\n1. ƒê·ªãnh nghƒ©a ƒë∆°n gi·∫£n\n2. T·∫°i sao c·∫ßn d√πng\n3. V√≠ d·ª• c·ª• th·ªÉ v·ªõi Docker Compose\n4. So s√°nh tr∆∞·ªõc/sau khi d√πng IaC\nT·ªëi ƒëa 200 t·ª´.",
     "Docker"),

    # ======== ADVANCED (15-20): Multi-tool & automation ========

    # Cron scheduling (cron tool ‚Äî real workflow)
    (15, "Adv",
     "M√¨nh c√≥ 3 cron job:\n- Job A: ch·∫°y m·ªói 15 ph√∫t (*/15 * * * *)\n- Job B: ch·∫°y ƒë·∫ßu m·ªói gi·ªù (0 * * * *)\n- Job C: ch·∫°y l√∫c 00:00 h√†ng ng√†y (0 0 * * *)\n\nT√≠nh:\n(a) T·ªïng s·ªë l·∫ßn ch·∫°y trong 24h cho m·ªói job\n(b) T·ªïng combined executions\n(c) Th·ªùi ƒëi·ªÉm n√†o Job A v√† Job B ch·∫°y ƒë·ªìng th·ªùi?\nTr√¨nh b√†y chi ti·∫øt t·ª´ng b∆∞·ªõc.",
     "96"),

    # Tool call generation (sessions_spawn/exec context)
    (16, "Adv",
     "B·∫°n l√† AI assistant c√≥ c√°c tool sau:\n- exec(command: string) ‚Äî ch·∫°y shell command\n- web_fetch(url: string) ‚Äî fetch n·ªôi dung URL\n- message(channel: string, text: string) ‚Äî g·ª≠i tin nh·∫Øn\n- cron(action: string, schedule: string, command: string) ‚Äî t·∫°o cron job\n\nUser y√™u c·∫ßu: 'Ki·ªÉm tra API https://api.example.com/health m·ªói 5 ph√∫t. N·∫øu down th√¨ g·ª≠i tin nh·∫Øn v√†o k√™nh #alerts.'\n\nGenerate tool calls d·∫°ng JSON array. Ch·ªâ output JSON, kh√¥ng gi·∫£i th√≠ch.",
     "cron"),

    # Complex coding (coding-agent advanced workflow)
    (17, "Adv",
     "Implement class TypeScript `TokenBucketRateLimiter`:\n- Constructor: maxTokens, refillRate (tokens/sec)\n- Method `tryConsume(n: number): boolean` ‚Äî true n·∫øu ƒë·ªß token\n- Method `getStatus(): {available, max, nextRefillMs}`\n- Auto-refill d·ª±a tr√™n th·ªùi gian th·ª±c\n- C√≥ usage example\nOutput production-ready code.",
     "tryConsume"),

    # Multi-step deployment (exec/read/write orchestration)
    (18, "Adv",
     "Thi·∫øt k·∫ø deployment plan zero-downtime cho Node.js app + PostgreSQL + Redis. Y√™u c·∫ßu:\n- Blue-green deployment\n- Database migration an to√†n\n- Rollback strategy\n- Health check\nƒê∆∞a ra ƒë√∫ng 7 b∆∞·ªõc, m·ªói b∆∞·ªõc g·ªìm: h√†nh ƒë·ªông, r·ªßi ro, c√°ch rollback.",
     "migration"),

    # Strict instruction following (system prompt compliance)
    (19, "Adv",
     "Tu√¢n th·ªß CH√çNH X√ÅC c√°c quy t·∫Øc sau:\n1. B·∫Øt ƒë·∫ßu response b·∫±ng 'REPORT:'\n2. Vi·∫øt markdown table 3 h√†ng, c·ªôt: Model, Speed, Quality, Cost\n3. H√†ng 1: GPT-4o, Fast, High, $$$\n4. H√†ng 2: Claude, Medium, Very High, $$\n5. H√†ng 3: DeepSeek, Fast, Good, $\n6. Sau table vi·∫øt ƒë√∫ng 1 c√¢u t·ªïng k·∫øt\n7. K·∫øt th√∫c b·∫±ng 'END_REPORT'\n8. KH√îNG th√™m b·∫•t k·ª≥ text n√†o tr∆∞·ªõc REPORT: ho·∫∑c sau END_REPORT",
     "REPORT:"),

    # Vietnamese complex plan (multi-tool orchestration context)
    (20, "Adv",
     "Vi·∫øt technical plan b·∫±ng ti·∫øng Vi·ªát ƒë·ªÉ chuy·ªÉn h·ªá th·ªëng t·ª´ monolith sang microservices. Y√™u c·∫ßu:\n1. Chia 5 giai ƒëo·∫°n\n2. M·ªói giai ƒëo·∫°n c√≥: m·ª•c ti√™u, c√¥ng vi·ªác ch√≠nh, r·ªßi ro, th·ªùi gian ∆∞·ªõc t√≠nh\n3. Format markdown table\n4. Bao g·ªìm services: Auth, Payment, Notification, Order\n5. T·ªïng th·ªùi gian kh√¥ng qu√° 6 th√°ng\nCh·ªâ output markdown table, kh√¥ng gi·∫£i th√≠ch th√™m.",
     "microservice"),
]

def call_model(model, question, q_id):
    """Call gateway /v1/chat/completions API."""
    payload = json.dumps({
        "model": model,
        "messages": [{"role": "user", "content": question}],
        "stream": False,
        "max_tokens": MAX_TOKENS,
    }).encode("utf-8")

    req = urllib.request.Request(
        f"{GATEWAY_URL}/v1/chat/completions",
        data=payload,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GW_TOKEN}",
        },
        method="POST",
    )

    start = time.time()
    try:
        with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
            body = json.loads(resp.read().decode("utf-8"))
            elapsed = round(time.time() - start, 2)
            content = body.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens = body.get("usage", {}).get("total_tokens", 0)
            return {
                "status": "OK",
                "content": content,
                "tokens": tokens,
                "time": elapsed,
                "model_used": body.get("model", model),
            }
    except Exception as e:
        elapsed = round(time.time() - start, 2)
        return {
            "status": f"FAIL: {str(e)[:80]}",
            "content": "",
            "tokens": 0,
            "time": elapsed,
            "model_used": model,
        }

# ---- Run ----
all_results = []

for model in MODELS:
    print(f"\n{'‚îÅ' * 60}")
    print(f"  Model: {model}")
    print(f"{'‚îÅ' * 60}")

    for q_id, category, question, keyword in QUESTIONS:
        result = call_model(model, question, q_id)
        has_kw = keyword.lower() in result["content"].lower() if result["status"] == "OK" else False

        icon = "‚úì" if has_kw else ("‚ñ≥" if result["status"] == "OK" else "‚úó")
        print(f"  {icon} Q{q_id:02d} [{category:5s}] {result['time']:6.1f}s | {result['tokens']:5d} tok | kw:{('Y' if has_kw else 'N')}")

        all_results.append({
            "model": model, "q_id": q_id, "category": category,
            "question": question[:80], "keyword": keyword,
            "status": result["status"], "has_keyword": has_kw,
            "time": result["time"], "tokens": result["tokens"],
            "content_preview": result["content"][:300],
        })

        safe_model = model.replace("/", "_")
        with open(f"{RESULTS_DIR}/{safe_model}_q{q_id:02d}.json", "w") as f:
            json.dump({"model": model, "q_id": q_id, "category": category,
                        "result": result}, f, indent=2, ensure_ascii=False)

        time.sleep(1)

# ---- Save raw ----
with open(f"{RESULTS_DIR}/all-results.json", "w") as f:
    json.dump(all_results, f, indent=2, ensure_ascii=False)

# ---- Generate report ----
models = MODELS
lines = []
lines.append("# OpenClaw Model Benchmark Report\n")
lines.append(f"**Date:** {time.strftime('%Y-%m-%d %H:%M')}")
lines.append(f"**Workflows:** {len(QUESTIONS)} | **Models:** {len(models)}")
lines.append(f"**Gateway:** localhost:3000 (OpenClaw)")
lines.append(f"**Max tokens:** {MAX_TOKENS}")
lines.append(f"**Levels:** Basic (Q1-7) | Intermediate (Q8-14) | Advanced (Q15-20)\n")

# Overall ranking
lines.append("## Overall Ranking\n")
lines.append("| # | Model | OK | Hit | Score | Avg Time | Tokens |")
lines.append("|---|-------|----|-----|-------|----------|--------|")

model_stats = {}
for m in models:
    rows = [r for r in all_results if r["model"] == m]
    ok = sum(1 for r in rows if r["status"] == "OK")
    kw = sum(1 for r in rows if r["has_keyword"])
    t = sum(r["time"] for r in rows)
    tok = sum(r["tokens"] for r in rows)
    avg = t / len(rows) if rows else 0
    score = round((kw / len(rows)) * 100, 1) if rows else 0
    model_stats[m] = {"ok": ok, "kw": kw, "score": score, "avg_t": round(avg, 1), "tok": tok}

medals = ["ü•á", "ü•à", "ü•â", "4.", "5.", "6."]
for rank, m in enumerate(sorted(models, key=lambda x: -model_stats[x]["score"]), 1):
    s = model_stats[m]
    short = m.split("/")[-1]
    lines.append(f"| {medals[rank-1]} | **{short}** | {s['ok']}/20 | {s['kw']}/20 | **{s['score']}%** | {s['avg_t']}s | {s['tok']:,} |")

# Per-question matrix
lines.append("\n## Per-Workflow Matrix\n")
short_names = [m.split("/")[-1] for m in models]
lines.append("| Q# | Level | " + " | ".join(short_names) + " |")
lines.append("|:---:|:-----:" + "|:---:" * len(models) + "|")

for q_id, cat, _, _ in QUESTIONS:
    row = f"| {q_id} | {cat} |"
    for m in models:
        r = next((x for x in all_results if x["model"] == m and x["q_id"] == q_id), None)
        if not r:
            row += " - |"
        elif r["status"] != "OK":
            row += " ‚úó |"
        elif r["has_keyword"]:
            row += f" ‚úì {r['time']}s |"
        else:
            row += f" ‚ñ≥ {r['time']}s |"
    lines.append(row)

lines.append("\n**Legend:** ‚úì keyword found | ‚ñ≥ answered, keyword missing | ‚úó API error\n")

# Speed
lines.append("## Speed Comparison\n")
lines.append("| Model | Min | Max | Avg | Median |")
lines.append("|-------|-----|-----|-----|--------|")
for m in models:
    ok_rows = [r for r in all_results if r["model"] == m and r["status"] == "OK"]
    if ok_rows:
        t = sorted([r["time"] for r in ok_rows])
        short = m.split("/")[-1]
        lines.append(f"| {short} | {min(t)}s | {max(t)}s | {round(sum(t)/len(t),1)}s | {t[len(t)//2]}s |")

# Level breakdown
lines.append("\n## Workflow Level Breakdown\n")
lines.append("| Model | Basic (1-7) | Mid (8-14) | Adv (15-20) | Total |")
lines.append("|-------|:-----------:|:----------:|:-----------:|:-----:|")
for m in models:
    short = m.split("/")[-1]
    parts = []
    total_kw = 0
    for cat in ["Basic", "Mid", "Adv"]:
        cat_rows = [r for r in all_results if r["model"] == m and r["category"] == cat]
        kw = sum(1 for r in cat_rows if r["has_keyword"])
        total_kw += kw
        parts.append(f"{kw}/{len(cat_rows)}")
    lines.append(f"| {short} | {parts[0]} | {parts[1]} | {parts[2]} | **{total_kw}/20** |")

lines.append("")

report = "\n".join(lines)
with open(f"{RESULTS_DIR}/benchmark-report.md", "w") as f:
    f.write(report)

print(f"\n{'=' * 60}")
print(f" BENCHMARK COMPLETE")
print(f"{'=' * 60}")
print(f" Report: {RESULTS_DIR}/benchmark-report.md")
print(f" Raw:    {RESULTS_DIR}/all-results.json")
print(f"{'=' * 60}")

print(f"\n RANKING:")
for rank, m in enumerate(sorted(models, key=lambda x: -model_stats[x]["score"]), 1):
    s = model_stats[m]
    short = m.split("/")[-1]
    bar = "‚ñà" * int(s["score"] / 5) + "‚ñë" * (20 - int(s["score"] / 5))
    print(f"  {rank}. {short:25s} {bar} {s['score']}% ({s['kw']}/20)")

PYEOF

echo ""
echo "Done! View: cat ./benchmark-results/benchmark-report.md"
